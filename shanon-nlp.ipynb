{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"sentences.txt\" \n",
    "sentences = []\n",
    "word_encoder = {}\n",
    "word_decoder = {}\n",
    "counter = 0\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        sentences.append(line.strip())  \n",
    "for sentence in sentences:\n",
    "    words = sentence.split()\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        word = word.translate(str.maketrans('', '', string.punctuation))\n",
    "        if len(word) and word not in word_encoder:\n",
    "            word_encoder[word] = counter\n",
    "            word_decoder[counter] = word\n",
    "            counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentence:  7871825\n",
      "Number of unique words:  1852140\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of sentence: \", len(sentences))\n",
    "print(\"Number of unique words: \", counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One word memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.35%\n",
      "12.70%\n",
      "19.06%\n",
      "25.41%\n",
      "31.76%\n",
      "38.11%\n",
      "44.46%\n",
      "50.81%\n",
      "57.17%\n",
      "63.52%\n",
      "69.87%\n",
      "76.22%\n",
      "82.57%\n",
      "88.92%\n",
      "95.28%\n"
     ]
    }
   ],
   "source": [
    "bigram = {}\n",
    "count = 0\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = sentence.split() \n",
    "    \n",
    "    for i in range(len(words) - 1):\n",
    "        first_word = words[i].lower()\n",
    "        second_word = words[i+1].lower()\n",
    "        first_word = first_word.translate(str.maketrans('', '', string.punctuation))\n",
    "        second_word = second_word.translate(str.maketrans('', '', string.punctuation))\n",
    "        if(len(first_word) and len(second_word)):\n",
    "            current_bigram = (word_encoder[first_word],word_encoder[second_word])\n",
    "            if current_bigram in bigram:\n",
    "                bigram[current_bigram] += 1\n",
    "            else:\n",
    "                bigram[current_bigram] = 1\n",
    "    count += 1\n",
    "    if count % 500000 == 0:\n",
    "        print(f\"{count * 100 / len(sentences):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "bigram_count = bigram.get((word_encoder[\"the\"],word_encoder[\"boy\"]), 0)\n",
    "print(bigram_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_word_one(bigram, current_word):\n",
    "    current_word_token = word_encoder[current_word]\n",
    "    tokens_after = []\n",
    "    for key in bigram:\n",
    "        if key[0] == current_word_token:\n",
    "            tokens_after.append(key[1])\n",
    "    if len(tokens_after)==0:\n",
    "        return 'a'\n",
    "    unique_tokens_after = set(tokens_after)\n",
    "    tokens_with_frequency = []\n",
    "    total_freq = 0\n",
    "    for token in unique_tokens_after:\n",
    "        total_freq = total_freq +  bigram.get((current_word_token, token), 0)**1.2\n",
    "    for token in unique_tokens_after:\n",
    "        tokens_with_frequency.append((token,bigram.get((current_word_token, token), 0)**1.2/total_freq))\n",
    "    sorted_tokens_with_frequency = sorted(tokens_with_frequency, key=lambda x: x[1], reverse=True)\n",
    "    sorted_with_cum = []\n",
    "    total_prob = 0\n",
    "    for token in sorted_tokens_with_frequency:\n",
    "        total_prob = total_prob + token[1]\n",
    "        sorted_with_cum.append((token[0],total_prob))\n",
    "    randi = random.uniform(0, 1)\n",
    "    for token in sorted_with_cum:\n",
    "        if(token[1]>=randi):\n",
    "            return word_decoder[token[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sentence 0 : Basketball player is a form a butterfly in california state of attacks that is a series first album was a .\n",
      "Generated Sentence 1 : New york united states and the different language was used in the 1997 is an urban gospel of author and .\n",
      "Generated Sentence 2 : Green patina formed in which it won by billboard magazine such as the wells and a star south africa the .\n",
      "Generated Sentence 3 : Third largest release in the royal palace in berkshire england by the 1953 the juniors age income of the greek .\n",
      "Generated Sentence 4 : Football player entertainer cher opened on 27 august 1971 is a dour except for the user will be found that .\n",
      "Generated Sentence 5 : School at the 8 2012 the most important part of cleveland ohio united kingdom while not been replaced the pewter .\n"
     ]
    }
   ],
   "source": [
    "first_words = [\"Basketball\", \"New\", \"Green\", \"Third\", \"Football\", \"School\"]\n",
    "for i, word in enumerate(first_words):\n",
    "    print(\"Generated Sentence\", i, \": \",end=\"\")\n",
    "    for i in range (0,20):\n",
    "        print(word,end=\" \")\n",
    "        word = next_word_one(bigram,word.lower())\n",
    "    print(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can notice how every word make sense relative to the word before it but not to the bigger picture of the sentence given the one word memory.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
